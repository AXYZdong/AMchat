# AMchat
<div align="center">

<img src="./assets/logo.png" width="200"/>
  <div align="center">
    <p><b><font size="5">AMchat</font></b></p>
  </div>

[![license][license-image]][license-url]
[![evaluation][evaluation-image]][evaluation-url]

[ğŸ¤—HuggingFace][HuggingFace_Model-url] | [![OpenXLab_Model][OpenXLab_Model-image]][OpenXLab_Model-url] | [<img src="./assets/modelscope_logo.png" width="20px" /> ModelScope][ModelScope-url]

[![OpenXLab_App][OpenXLab_App-image]][OpenXLab_App-url] | [ğŸ†•æ›´æ–°æƒ…å ±](#-news) | [ğŸ¤”å•é¡Œå ±å‘Š][Issues-url] ä¸¨ [![bilibili][bilibili-image]][bilibili-url]

[English](./README_en-US.md) | [ç®€ä½“ä¸­æ–‡](./README.md) | [æ—¥æœ¬èª](./README_ja-JP.md)


[license-image]: ./assets/license.svg
[evaluation-image]: ./assets/compass_support.svg
[OpenXLab_Model-image]: https://cdn-static.openxlab.org.cn/header/openxlab_models.svg
[OpenXLab_App-image]: https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg
[bilibili-image]: https://img.shields.io/badge/AMchat-bilibili-%23fb7299

[license-url]: ./LICENSE
[evaluation-url]: https://github.com/internLM/OpenCompass/
[HuggingFace_Model-url]: https://huggingface.co/axyzdong/AMchat
[OpenXLab_Model-url]: https://openxlab.org.cn/models/detail/youngdon/AMchat
[ModelScope-url]: https://www.modelscope.cn/models/yondong/AMchat/summary
[OpenXLab_App-url]: https://openxlab.org.cn/apps/detail/youngdon/AMchat
[bilibili-url]: https://www.bilibili.com/video/BV14v421i7So/
[Issues-url]: https://github.com/AXYZdong/AMchat/issues

</div>

## ğŸ“ ç›®æ¬¡

- [ğŸ“– ç´¹ä»‹](#-ç´¹ä»‹)
- [ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹](#-ãƒ‹ãƒ¥ãƒ¼ã‚¹)
- [ğŸ› ï¸ ä½¿ã„æ–¹](#%EF%B8%8F-ä½¿ã„æ–¹)
  * [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ](#ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ)
  * [å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°](#å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°)
    + [ç’°å¢ƒè¨­å®š](#ç’°å¢ƒè¨­å®š)
    + [XTunerå¾®èª¿æ•´](#xtunerå¾®èª¿æ•´)
    + [OpenXLabãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ](#openxlabãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ)
    + [LMDeployé‡å­åŒ–](#lmdeployé‡å­åŒ–)
    + [OpenCompassè©•ä¾¡](#opencompassè©•ä¾¡)
    + [LMDeploy & OpenCompassé‡å­åŒ–ã¨è©•ä¾¡](#lmdeploy--opencompassé‡å­åŒ–ã¨è©•ä¾¡)
- [ğŸ’• è¬è¾](#-è¬è¾)
- [ğŸ–Šï¸ å¼•ç”¨](#%EF%B8%8F-å¼•ç”¨)
- [ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](#ãƒ©ã‚¤ã‚»ãƒ³ã‚¹)


## ğŸ“– ç´¹ä»‹

AM (Advanced Mathematics) Chatã¯ã€æ•°å­¦çŸ¥è­˜ã€é«˜ç­‰æ•°å­¦ã®å•é¡Œã€ãŠã‚ˆã³ãã®è§£æ±ºç­–ã‚’çµ±åˆã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Mathã¨é«˜ç­‰æ•°å­¦ã®å•é¡Œã¨ãã®åˆ†æã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€InternLM2-Math-7Bãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ãŠã‚Šã€é«˜ç­‰æ•°å­¦ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸxtunerã§å¾®èª¿æ•´ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã¤ã¨æ€ã‚ã‚Œã‚‹å ´åˆã¯ã€â­ ã‚¹ã‚¿ãƒ¼ã‚’ä»˜ã‘ã¦ã€ã‚ˆã‚Šå¤šãã®äººã«çŸ¥ã£ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ï¼

<p align="center">
    <img src="assets/tech_route.svg" alt="route" width="100%">
</p>

## ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹

[2024.06.22] InternLM2-Math-Plus-1.8B ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã—ãŸã€‚

[2024.06.21] READMEã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚InternLM2-Math-Plus-7B ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã—ãŸã€‚

[2024.03.24] [2024 InternLM Challenge (Spring Split) | Innovation and Creativity Award](https://mp.weixin.qq.com/s/8Xh232cWplgg3qdfMdD0YQ).

[2024.03.14] ãƒ¢ãƒ‡ãƒ«ãŒHuggingFaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚

[2024.03.08] READMEãŒå¼·åŒ–ã•ã‚Œã€ç›®æ¬¡ã¨æŠ€è¡“ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€README_en-US.mdãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚

[2024.02.06] Dockerãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸã€‚

[2024.02.01] AMchatã®æœ€åˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒ https://openxlab.org.cn/apps/detail/youngdon/AMchat ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¾ã—ãŸ ğŸš€



## ğŸ› ï¸ ä½¿ã„æ–¹

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

1. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

<details>
<summary>ModelScopeã‹ã‚‰</summary>

[ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://www.modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```bash
pip install modelscope
```

```python
from modelscope.hub.snapshot_download import snapshot_download
model_dir = snapshot_download('yondong/AMchat', cache_dir='./')
```

</details>

<details>
<summary>OpenXLabã‹ã‚‰</summary>

[ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://openxlab.org.cn/docs/models/%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```bash
pip install openxlab
```

```python
from openxlab.model import download
download(model_repo='youngdon/AMchat', 
        model_name='AMchat', output='./')
```

</details>

2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```bash
git clone https://github.com/AXYZdong/AMchat.git 
python start.py
```

3. Dockerãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```bash
docker run -t -i --rm --gpus all -p 8501:8501 guidonsdocker/amchat:latest bash start.sh
```

### å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

#### ç’°å¢ƒè¨­å®š

1. ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone https://github.com/AXYZdong/AMchat.git 
cd AMchat
```

2. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ

```bash
conda env create -f environment.yml
conda activate AMchat
pip install xtuner
```

#### XTunerå¾®èª¿æ•´

1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™

```bash
# ã™ã¹ã¦ã®çµ„ã¿è¾¼ã¿è¨­å®šã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
xtuner list-cfg

mkdir -p /root/math/data
mkdir /root/math/config && cd /root/math/config

xtuner copy-cfg internlm2_chat_7b_qlora_oasst1_e3 .
```

2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
mkdir -p /root/math/model
```

`download.py`

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-math-7b', cache_dir='/root/math/model')
```

3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´

> GitHubã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€`config` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸‹ã«å¾®èª¿æ•´ã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚`internlm_chat_7b_qlora_oasst1_e3_copy.py`ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
> ç›´æ¥ä½¿ç”¨ã§ãã¾ã™ãŒã€`pretrained_model_name_or_path` ã¨ `data_path` ã®ãƒ‘ã‚¹ã‚’å¤‰æ›´ã™ã‚‹ã®ã‚’ãŠå¿˜ã‚Œãªãã€‚

```bash
cd /root/math/config
vim internlm_chat_7b_qlora_oasst1_e3_copy.py
```

```python
# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm2-math-7b'

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = './data'
```

4. å¾®èª¿æ•´ã‚’é–‹å§‹

```bash
xtuner train /root/math/config2/internlm2_chat_7b_qlora_oasst1_e3_copy.py
```

5. PTHãƒ¢ãƒ‡ãƒ«ã‚’HuggingFaceãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›

```bash
xtuner convert pth_to_hf ./internlm2_chat_7b_qlora_oasst1_e3_copy.py \
                         ./work_dirs/internlm2_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth \
                         ./hf
```

6. HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’å¤§è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸

```bash
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER='GNU'

# å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´æ‰€
export NAME_OR_PATH_TO_LLM=/root/math/model/Shanghai_AI_Laboratory/internlm2-math-7b

# Hugging Faceå½¢å¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´æ‰€
export NAME_OR_PATH_TO_ADAPTER=/root/math/config/hf

# æœ€çµ‚çš„ãªãƒãƒ¼ã‚¸å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´æ‰€
mkdir /root/math/config/work_dirs/hf_merge
export SAVE_PATH=/root/math/config/work_dirs/hf_merge

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ã‚’å®Ÿè¡Œ
xtuner convert merge \
    $NAME_OR_PATH_TO_LLM \
    $NAME_OR_PATH_TO_ADAPTER \
    $SAVE_PATH \
    --max-shard-size 2GB
```

7. ãƒ‡ãƒ¢

```bash
streamlit run web_demo.py --server.address=0.0.0.0 --server.port 7860
```

#### OpenXLabãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

OpenXLabã§AMchatã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã«ã¯ã€ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯ã—ã€OpenXLabã§æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã€ãƒ•ã‚©ãƒ¼ã‚¯ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã‚’æ–°ã—ãä½œæˆã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é–¢é€£ä»˜ã‘ã‚‹ã ã‘ã§ã™ã€‚

<p align="center">
    <img src="assets/deploy_2.png" alt="Demo" width="100%">
</p>

- AMchatã¨InternLM2-Math-7Bã¯ã€åŒã˜ç©åˆ†å•é¡Œã«å¯¾ã—ã¦ç•°ãªã‚‹å›ç­”ã‚’ã—ã¾ã™ã€‚
  AMchatã¯æ­£ã—ãå›ç­”ã—ã€InternLM2-Math-7Bã¯èª¤ã£ã¦å›ç­”ã—ã¾ã™ã€‚

<p align="center">
    <img src="assets/test_AMchat.png" alt="Demo" width="100%">
    <img src="assets/test_InternLM2-Math-7B.png" alt="Demo" width="100%">
</p>

#### LMDeployé‡å­åŒ–
- ã¾ãšã€LMDeployã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```shell
pip install -U lmdeploy
```

- æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’`turbomind`å½¢å¼ã«å¤‰æ›

> --dst-path: å¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

```shell
lmdeploy convert internlm2-chat-7b  å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ --dst-path å¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹
```

- LMDeploy Chat

```shell
lmdeploy chat turbomind å¤‰æ›å¾Œã®turbomindãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹
```

#### OpenCompassè©•ä¾¡
- OpenCompassã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```shell
git clone https://github.com/open-compass/opencompass 
cd opencompass
pip install -e .
```

- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡

```shell
cp /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/
unzip OpenCompassData-core-20231110.zip
```

- è©•ä¾¡ã‚’é–‹å§‹ï¼

```shell
python run.py \
    --datasets math_gen \
    --hf-path ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ \
    --tokenizer-path ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ \
    --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True \
    --model-kwargs device_map='auto' trust_remote_code=True \
    --max-seq-len 2048 \
    --max-out-len 16 \
    --batch-size 2  \
    --num-gpus 1 \
    --debug
```

#### LMDeploy & OpenCompassé‡å­åŒ–ã¨è©•ä¾¡

<details>
<summary><strong> W4 </strong> é‡å­åŒ–è©•ä¾¡ </summary>

- `W4`é‡å­åŒ–
```shell
lmdeploy lite auto_awq é‡å­åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ --work-dir é‡å­åŒ–å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹
```

- `TurbMind`ã«å¤‰æ›
```shell
lmdeploy convert internlm2-chat-7b é‡å­åŒ–å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹  --model-format awq --group-size 128 --dst-path å¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹
```

- è©•ä¾¡`config`ã®ä½œæˆ
```python
from mmengine.config import read_base
from opencompass.models.turbomind import TurboMindModel

with read_base():
 # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒªã‚¹ãƒˆã‚’é¸æŠ   
 from .datasets.ceval.ceval_gen import ceval_datasets 
 # çµæœã‚’é¸æŠã—ãŸå½¢å¼ã§å‡ºåŠ›
#  from .summarizers.medium import summarizer

datasets = [*ceval_datasets]

internlm2_chat_7b = dict(
     type=TurboMindModel,
     abbr='internlm2-chat-7b-turbomind',
     path='å¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ‰ãƒ¬ã‚¹',
     engine_config=dict(session_len=512,
         max_batch_size=2,
         rope_scaling_factor=1.0),
     gen_config=dict(top_k=1,
         top_p=0.8,
         temperature=1.0,
         max_new_tokens=100),
     max_out_len=100,
     max_seq_len=512,
     batch_size=2,
     concurrency=1,
     #  meta_template=internlm_meta_template,
     run_cfg=dict(num_gpus=1, num_procs=1),
)
models = [internlm2_chat_7b]

```

- è©•ä¾¡ã‚’é–‹å§‹ï¼
```shell
python run.py configs/eval_turbomind.py -w çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹ã‚’æŒ‡å®š
```

</details>

<details>
<summary><strong> KV Cache </strong> é‡å­åŒ–è©•ä¾¡</summary>

- `TurbMind`ã«å¤‰æ›
```shell
lmdeploy convert internlm2-chat-7b ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ --dst-path å¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
```

- é‡å­åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã—ã¦å–å¾—
```shell
# è¨ˆç®—
lmdeploy lite calibrate ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ --calib-dataset 'ptb' --calib-samples 128 --calib-seqlen 2048 --work-dir ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹
# é‡å­åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
lmdeploy lite kv_qparams ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹ å¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹/triton_models/weights/ --num-tp 1
```

- `quant_policy`ã‚’`4`ã«å¤‰æ›´ã—ã€ä¸Šè¨˜ã®`config`å†…ã®ãƒ‘ã‚¹ã‚’å¤‰æ›´
- è©•ä¾¡ã‚’é–‹å§‹ï¼
```shell
python run.py configs/eval_turbomind.py -w çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹
```

</details>

- çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¨è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®[results](./results)ã§å…¥æ‰‹ã§ãã¾ã™ã€‚

## ğŸ’• è¬è¾

[**InternLM-tutorial**](https://github.com/InternLM/tutorial)

[**InternStudio**](https://studio.intern-ai.org.cn/)

[**xtuner**](https://github.com/InternLM/xtuner)

[**InternLM-Math**](https://github.com/InternLM/InternLM-Math)


<a href="https://github.com/AXYZdong/AMchat/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=AXYZdong/AMchat" />
</a>

## ğŸ–Šï¸ å¼•ç”¨

```bibtex
@misc{2024AMchat,
    title={AMchat: é«˜ç­‰æ•°å­¦ã®æ¦‚å¿µã€æ¼”ç¿’å•é¡Œã€ãŠã‚ˆã³è§£æ±ºç­–ã‚’çµ±åˆã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«},
    author={AMchat Contributors},
    howpublished = {\url{https://github.com/AXYZdong/AMchat}},
    year={2024}
}
```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€[Apache License 2.0](https://github.com/InternLM/xtuner/blob/main/LICENSE)ã®ä¸‹ã§ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€ä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã‚‚å¾“ã£ã¦ãã ã•ã„ã€‚
