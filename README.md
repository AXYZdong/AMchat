# AMchat é«˜ç­‰æ•°å­¦å¤§æ¨¡å‹
<div align="center">

<img src="./assets/logo.png" width="200"/>
  <div align="center">
    <b><font size="5">AMchat</font></b>
  </div>

[![license][license-image]][license-url]
[![evaluation][evaluation-image]][evaluation-url]

[ğŸ¤—HuggingFace][HuggingFace_Model-url] | [![OpenXLab_Model][OpenXLab_Model-image]][OpenXLab_Model-url] | [<img src="./assets/modelscope_logo.png" width="20px" /> ModelScope][ModelScope-url] | [ğŸ¤—HuggingFace-GGUF][HuggingFace_Model-GGUF-url]

[![OpenXLab_App][OpenXLab_App-image]][OpenXLab_App-url] | [ğŸ†•Update News](#-news) | [ğŸ¤”Reporting Issues][Issues-url] ä¸¨ [![bilibili][bilibili-image]][bilibili-url]

[English](./README_en-US.md) | [ç®€ä½“ä¸­æ–‡](./README.md) | [æ—¥æœ¬èª](./README_ja-JP.md)


[license-image]: ./assets/license.svg
[evaluation-image]: ./assets/compass_support.svg
[OpenXLab_Model-image]: https://cdn-static.openxlab.org.cn/header/openxlab_models.svg
[OpenXLab_App-image]: https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg
[bilibili-image]: https://img.shields.io/badge/AMchat-bilibili-%23fb7299

[license-url]: ./LICENSE
[evaluation-url]: https://github.com/internLM/OpenCompass/
[HuggingFace_Model-url]: https://huggingface.co/axyzdong/AMchat
[HuggingFace_Model-GGUF-url]: https://huggingface.co/axyzdong/AMchat-GGUF
[OpenXLab_Model-url]: https://openxlab.org.cn/models/detail/youngdon/AMchat
[ModelScope-url]: https://www.modelscope.cn/models/yondong/AMchat/summary
[OpenXLab_App-url]: https://openxlab.org.cn/apps/detail/youngdon/AMchat
[bilibili-url]: https://www.bilibili.com/video/BV14v421i7So/
[Issues-url]: https://github.com/AXYZdong/AMchat/issues

</div>

## ğŸ“ç›®å½•

- [ğŸ“– ç®€ä»‹](#-ç®€ä»‹)
- [ğŸš€ News](#-news)
- [ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•](#%EF%B8%8F-ä½¿ç”¨æ–¹æ³•)
  * [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
  * [é‡æ–°è®­ç»ƒ](#é‡æ–°è®­ç»ƒ)
    + [ç¯å¢ƒæ­å»º](#ç¯å¢ƒæ­å»º)
    + [XTunerå¾®è°ƒ](#xtunerå¾®è°ƒ)
    + [OpenXLabåº”ç”¨éƒ¨ç½²](#openxlabåº”ç”¨éƒ¨ç½²)
    + [LMDeployé‡åŒ–](#lmdeployé‡åŒ–)
    + [OpenCompassè¯„æµ‹](#opencompassè¯„æµ‹)
    + [LMDeploy & OpenCompassé‡åŒ–ä»¥åŠé‡åŒ–è¯„æµ‹](#lmdeploy--opencompassé‡åŒ–ä»¥åŠé‡åŒ–è¯„æµ‹)
- [ğŸ’• è‡´è°¢](#-è‡´è°¢)
- [ğŸ–Šï¸ Citation](#%EF%B8%8F-citation)
- [å¼€æºè®¸å¯è¯](#å¼€æºè®¸å¯è¯)


## ğŸ“– ç®€ä»‹

AM (Advanced Mathematics) chat æ˜¯ä¸€ä¸ªé›†æˆäº†æ•°å­¦çŸ¥è¯†å’Œé«˜ç­‰æ•°å­¦ä¹ é¢˜åŠå…¶è§£ç­”çš„å¤§è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ Math å’Œé«˜ç­‰æ•°å­¦ä¹ é¢˜åŠå…¶è§£æèåˆçš„æ•°æ®é›†ï¼ŒåŸºäº InternLM2-Math-7B æ¨¡å‹ï¼Œé€šè¿‡ xtuner å¾®è°ƒï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§£ç­”é«˜ç­‰æ•°å­¦é—®é¢˜ã€‚

å¦‚æœä½ è§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ â­ Starï¼Œè®©æ›´å¤šçš„äººå‘ç°å®ƒï¼

<p align="center">
    <img src="assets/tech_route.svg" alt="route" width="100%">
</p>

## ğŸš€ News

[2024.08.09] æˆ‘ä»¬å‘å¸ƒäº†Q8_0é‡åŒ–æ¨¡å‹ [AMchat-q8_0.gguf](https://huggingface.co/axyzdong/AMchat-GGUF)ã€‚

[2024.06.23] [InternLM2-Math-Plus-20B æ¨¡å‹å¾®è°ƒ](plus/03-InternLM2-Math-Plus-20B%20å¾®è°ƒ.md)ã€‚

[2024.06.22] [InternLM2-Math-Plus-1.8B æ¨¡å‹å¾®è°ƒ](plus/01-InternLM2-Math-Plus-1.8B%20å¾®è°ƒ.md)ï¼Œå¼€æº[å°è§„æ¨¡æ•°æ®é›†](dataset/AMchat_dataset.json)ã€‚

[2024.06.21] æ›´æ–°READMEï¼Œ[InternLM2-Math-Plus-7B æ¨¡å‹å¾®è°ƒ](plus/02-InternLM2-Math-Plus-7B%20å¾®è°ƒ.md)ã€‚

[2024.03.24] [2024æµ¦æºå¤§æ¨¡å‹ç³»åˆ—æŒ‘æˆ˜èµ›ï¼ˆæ˜¥å­£èµ›ï¼‰Top12](https://mp.weixin.qq.com/s/8Xh232cWplgg3qdfMdD0YQ)ï¼Œåˆ›æ–°åˆ›æ„å¥–ã€‚

[2024.03.14] æ¨¡å‹ä¸Šä¼ è‡³HuggingFaceã€‚

[2024.03.08] å®Œå–„äº†READMEï¼Œå¢åŠ ç›®å½•ã€æŠ€æœ¯è·¯çº¿ã€‚å¢åŠ README_en-US.mdã€‚

[2024.02.06] æ”¯æŒäº†Dockeréƒ¨ç½²ã€‚

[2024.02.01] AMchatç¬¬ä¸€ç‰ˆéƒ¨ç½²ä¸Šçº¿ https://openxlab.org.cn/apps/detail/youngdon/AMchat ğŸš€


## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

1. ä¸‹è½½æ¨¡å‹

<details>
<summary> ä» ModelScope </summary>

å‚è€ƒ [æ¨¡å‹çš„ä¸‹è½½](https://www.modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD) ã€‚

```bash
pip install modelscope
```

```python
from modelscope.hub.snapshot_download import snapshot_download
model_dir = snapshot_download('yondong/AMchat', cache_dir='./')
```

</details>


<details>
<summary> ä» OpenXLab </summary>

å‚è€ƒ [ä¸‹è½½æ¨¡å‹](https://openxlab.org.cn/docs/models/%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B.html) ã€‚

```bash
pip install openxlab
```

```python
from openxlab.model import download
download(model_repo='youngdon/AMchat', 
        model_name='AMchat', output='./')
```

</details>

2. æœ¬åœ°éƒ¨ç½²

```bash
git clone https://github.com/AXYZdong/AMchat.git
python start.py
```

3. Dockeréƒ¨ç½²

```bash
docker run -t -i --rm --gpus all -p 8501:8501 guidonsdocker/amchat:latest bash start.sh
```

### é‡æ–°è®­ç»ƒ

#### ç¯å¢ƒæ­å»º

1. clone æœ¬é¡¹ç›®

```bash
git clone https://github.com/AXYZdong/AMchat.git
cd AMchat
```

2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
conda env create -f environment.yml
conda activate AMchat
pip install xtuner
```

#### XTunerå¾®è°ƒ

1. å‡†å¤‡é…ç½®æ–‡ä»¶

```bash
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®
xtuner list-cfg

mkdir -p /root/math/data
mkdir /root/math/config && cd /root/math/config

xtuner copy-cfg internlm2_chat_7b_qlora_oasst1_e3 .
```

2. æ¨¡å‹ä¸‹è½½

```bash
mkdir -p /root/math/model
```
`download.py`

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-math-7b', cache_dir='/root/math/model')
```


3. ä¿®æ”¹é…ç½®æ–‡ä»¶

> ä»“åº“ä¸­ `config` æ–‡ä»¶å¤¹ä¸‹å·²ç»æä¾›äº†ä¸€ä¸ªå¾®è°ƒçš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥å‚è€ƒ `internlm_chat_7b_qlora_oasst1_e3_copy.py`ã€‚
> å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œæ³¨æ„ä¿®æ”¹  `pretrained_model_name_or_path` å’Œ `data_path` çš„è·¯å¾„ã€‚


```bash
cd /root/math/config
vim internlm_chat_7b_qlora_oasst1_e3_copy.py
```

```python
# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm2-math-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®é›†ä¸ºæœ¬åœ°è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = './data'
```

4. å¼€å§‹å¾®è°ƒ

```bash
xtuner train /root/math/config/internlm2_chat_7b_qlora_oasst1_e3_copy.py
```

5. PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹

```bash
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU
xtuner convert pth_to_hf ./internlm2_chat_7b_qlora_oasst1_e3_copy.py \
                         ./work_dirs/internlm2_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth \
                         ./hf
```

6. HuggingFace æ¨¡å‹åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹
```bash
# åŸå§‹æ¨¡å‹å‚æ•°å­˜æ”¾çš„ä½ç½®
export NAME_OR_PATH_TO_LLM=/root/math/model/Shanghai_AI_Laboratory/internlm2-math-7b

# Hugging Faceæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
export NAME_OR_PATH_TO_ADAPTER=/root/math/config/hf

# æœ€ç»ˆMergeåçš„å‚æ•°å­˜æ”¾çš„ä½ç½®
mkdir /root/math/config/work_dirs/hf_merge
export SAVE_PATH=/root/math/config/work_dirs/hf_merge

# æ‰§è¡Œå‚æ•°Merge
xtuner convert merge \
    $NAME_OR_PATH_TO_LLM \
    $NAME_OR_PATH_TO_ADAPTER \
    $SAVE_PATH \
    --max-shard-size 2GB
```

7. Demo

```bash
streamlit run web_demo.py --server.address=0.0.0.0 --server.port 7860
```

#### OpenXLabåº”ç”¨éƒ¨ç½²

ä»…éœ€è¦ Fork æœ¬ä»“åº“ï¼Œç„¶ååœ¨ OpenXLab ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„é¡¹ç›®ï¼Œå°† Fork çš„ä»“åº“ä¸æ–°å»ºçš„é¡¹ç›®å…³è”ï¼Œå³å¯åœ¨ OpenXLab ä¸Šéƒ¨ç½² AMchatã€‚

<p align="center">
    <img src="assets/deploy_2.png" alt="Demo" width="100%">
</p>

- AMchat ä¸ InternLM2-Math-7B åœ¨ç§¯åˆ†é—®é¢˜ä¸Šå¯¹äºåŒä¸€é—®é¢˜çš„è§£ç­”ã€‚ 
  AMchat å›ç­”æ­£ç¡®ï¼ŒInternLM2-Math-7B å›ç­”é”™è¯¯ã€‚
  
<p align="center">
    <img src="assets/test_AMchat.png" alt="Demo" width="100%">
    <img src="assets/test_InternLM2-Math-7B.png" alt="Demo" width="100%">
</p>

#### LMDeployé‡åŒ–
- é¦–å…ˆå®‰è£…LMDeploy

```shell
pip install -U lmdeploy
```

- ç„¶åè½¬æ¢æ¨¡å‹ä¸º`turbomind`æ ¼å¼

> --dst-path: å¯ä»¥æŒ‡å®šè½¬æ¢åçš„æ¨¡å‹å­˜å‚¨ä½ç½®ã€‚

```shell
lmdeploy convert internlm2-chat-7b  è¦è½¬åŒ–çš„æ¨¡å‹åœ°å€ --dst-path è½¬æ¢åçš„æ¨¡å‹åœ°å€
```

- LMDeploy Chat å¯¹è¯

```shell
lmdeploy chat turbomind è½¬æ¢åçš„turbomindæ¨¡å‹åœ°å€
```
#### OpenCompassè¯„æµ‹
- å®‰è£… OpenCompass

```shell
git clone https://github.com/open-compass/opencompass
cd opencompass
pip install -e .
```

- ä¸‹è½½è§£å‹æ•°æ®é›†

```shell
cp /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/
unzip OpenCompassData-core-20231110.zip
```

- è¯„æµ‹å¯åŠ¨ï¼

```shell
python run.py \
    --datasets math_gen \
    --hf-path æ¨¡å‹åœ°å€ \
    --tokenizer-path tokenizeråœ°å€ \
    --tokenizer-kwargs padding_side='left' truncation='left'     trust_remote_code=True \
    --model-kwargs device_map='auto' trust_remote_code=True \
    --max-seq-len 2048 \
    --max-out-len 16 \
    --batch-size 2  \
    --num-gpus 1 \
    --debug
```
  
#### LMDeploy & OpenCompassé‡åŒ–ä»¥åŠé‡åŒ–è¯„æµ‹  

<details>
<summary><strong> W4 </strong> é‡åŒ–è¯„æµ‹ </summary>

- `W4`é‡åŒ–
```shell
lmdeploy lite auto_awq è¦é‡åŒ–çš„æ¨¡å‹åœ°å€ --work-dir é‡åŒ–åçš„æ¨¡å‹åœ°å€
```

- è½¬åŒ–ä¸º`TurbMind`
```shell
lmdeploy convert internlm2-chat-7b é‡åŒ–åçš„æ¨¡å‹åœ°å€  --model-format awq --group-size 128 --dst-path è½¬æ¢åçš„æ¨¡å‹åœ°å€
```

- è¯„æµ‹`config`ç¼–å†™  

```python
from mmengine.config import read_base
from opencompass.models.turbomind import TurboMindModel

with read_base():
 # choose a list of datasets   
 from .datasets.ceval.ceval_gen import ceval_datasets 
 # and output the results in a choosen format
#  from .summarizers.medium import summarizer

datasets = [*ceval_datasets]

internlm2_chat_7b = dict(
     type=TurboMindModel,
     abbr='internlm2-chat-7b-turbomind',
     path='è½¬æ¢åçš„æ¨¡å‹åœ°å€',
     engine_config=dict(session_len=512,
         max_batch_size=2,
         rope_scaling_factor=1.0),
     gen_config=dict(top_k=1,
         top_p=0.8,
         temperature=1.0,
         max_new_tokens=100),
     max_out_len=100,
     max_seq_len=512,
     batch_size=2,
     concurrency=1,
     #  meta_template=internlm_meta_template,
     run_cfg=dict(num_gpus=1, num_procs=1),
)
models = [internlm2_chat_7b]

```

- è¯„æµ‹å¯åŠ¨ï¼
```shell
python run.py configs/eval_turbomind.py -w æŒ‡å®šç»“æœä¿å­˜è·¯å¾„
```

</details>

<details>
<summary> <strong> KV Cache </strong> é‡åŒ–è¯„æµ‹ </summary>

- è½¬æ¢ä¸º`TurbMind`
```shell
lmdeploy convert internlm2-chat-7b  æ¨¡å‹è·¯å¾„ --dst-path è½¬æ¢åæ¨¡å‹è·¯å¾„
```
- è®¡ç®—ä¸è·å¾—é‡åŒ–å‚æ•°
```shell
# è®¡ç®—
lmdeploy lite calibrate æ¨¡å‹è·¯å¾„ --calib-dataset 'ptb' --calib-samples 128 --calib-seqlen 2048 --work-dir å‚æ•°ä¿å­˜è·¯å¾„
# è·å–é‡åŒ–å‚æ•°
lmdeploy lite kv_qparams å‚æ•°ä¿å­˜è·¯å¾„ è½¬æ¢åæ¨¡å‹è·¯å¾„/triton_models/weights/ --num-tp 1
```
- æ›´æ”¹`quant_policy`æ”¹æˆ`4`,æ›´æ”¹ä¸Šè¿°`config`é‡Œé¢çš„è·¯å¾„
- è¯„æµ‹å¯åŠ¨ï¼
```shell
python run.py configs/eval_turbomind.py -w ç»“æœä¿å­˜è·¯å¾„
```

</details>

- ç»“æœæ–‡ä»¶ä¸è¯„æµ‹æ•°æ®é›†å¯åœ¨åŒç›®å½•æ–‡ä»¶[results](./results)ä¸­è·å–ã€‚


## ğŸ’• è‡´è°¢

### é¡¹ç›®æˆå‘˜

- å¼ å‹ä¸œ-é¡¹ç›®è´Ÿè´£äºº ï¼ˆDatawhaleæˆå‘˜ ä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥åŠ©æ•™ è´Ÿè´£æ¨¡å‹è®­ç»ƒï¼ŒOpenXlabåº”ç”¨éƒ¨ç½²ï¼Œæ•°æ®æ”¶é›†ï¼ŒRAGå†…å®¹æ•´ç†ï¼ŒInternLM2-Math-Pluså¾®è°ƒè§„åˆ’ï¼‰
- å®‹å¿—å­¦-é¡¹ç›®è´Ÿè´£äºº ï¼ˆDatawhaleæˆå‘˜ ä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥åŠ©æ•™ è´Ÿè´£é¡¹ç›®è§„åˆ’ï¼ŒRAGæ¡†æ¶ï¼‰
- è‚–é¸¿å„’-é¡¹ç›®è´Ÿè´£äºº ï¼ˆDatawhaleæˆå‘˜ åŒæµå¤§å­¦ ä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥åŠ©æ•™ è´Ÿè´£æ•°æ®æ”¶é›†ï¼Œæ•°æ®é›†æ•´ç†åŠå¢å¼ºï¼Œæ¨¡å‹é‡åŒ–ä¸è¯„æµ‹ï¼ŒRAGæ¨ç†ä¸éªŒè¯ï¼‰
- ç¨‹å® ï¼ˆä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥åŠ©æ•™&Datawhaleé²¸è‹±åŠ©æ•™ InternLM2-Math-Plus-7B æ¨¡å‹å¾®è°ƒ&éƒ¨ç½²ï¼‰
- è«å®çªï¼ˆç‰æŸ´å·¥ç¨‹ç ”ç©¶é™¢ InternLM2-Math-Plus-1.8B æ¨¡å‹å¾®è°ƒï¼‰
- é™ˆè¾…å…ƒï¼ˆç”˜è‚ƒæ”¿æ³•å¤§å­¦ InternLM2-Math-Plus-20B æ¨¡å‹å¾®è°ƒï¼‰
- é¾šé¹¤æ‰¬ ï¼ˆä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ç»Ÿè®¡å­¦åšå£« LMDeploy æ¨¡å‹é‡åŒ–ï¼‰
- æ­ç†”é˜³ ï¼ˆDatawhaleæˆå‘˜ å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦(å¨æµ·) æ•°æ®æ”¶é›† RAGå†…å®¹æ•´ç†ï¼‰
- å½­ç› ï¼ˆDatawhaleæˆå‘˜ æ•°æ®æ”¶é›†ï¼‰
- ç‹æ–°èŒ— ï¼ˆæ•°æ®æ”¶é›†ï¼‰
- åˆ˜å¿—æ–‡ ï¼ˆDatawhaleæˆå‘˜ å±±ä¸œå¥³å­å­¦é™¢ æ•°æ®æ”¶é›†ï¼‰
- ç‹ç¿ç¥ ï¼ˆNortheastern University æ•°æ®æ”¶é›†ï¼‰
- é™ˆé€¸æ¶µ ï¼ˆDatawhaleæˆå‘˜ åŒ—äº¬é‚®ç”µå¤§å­¦ æ•°æ®æ”¶é›†ï¼‰
- guidons ï¼ˆä¸œåŒ—å¤§å­¦ dockeréƒ¨ç½²ï¼‰
- eltociear ï¼ˆBoard member at I-Tecnology Co., Ltd.ï¼Œå¢åŠ  Japanese READMEï¼‰

### ç‰¹åˆ«é¸£è°¢

<div align="center">

***æ„Ÿè°¢ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ç»„ç»‡çš„ ä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥ å­¦ä¹ æ´»åŠ¨~***

***æ„Ÿè°¢ OpenXLab å¯¹é¡¹ç›®éƒ¨ç½²çš„ç®—åŠ›æ”¯æŒ~***

***æ„Ÿè°¢ æµ¦è¯­å°åŠ©æ‰‹ å¯¹é¡¹ç›®çš„æ”¯æŒ~***

***æ„Ÿè°¢ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ¨å‡ºçš„ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ï¼Œä¸ºæˆ‘ä»¬çš„é¡¹ç›®æä¾›å®è´µçš„æŠ€æœ¯æŒ‡å¯¼å’Œå¼ºå¤§çš„ç®—åŠ›æ”¯æŒï¼***

[**InternLM-tutorial**](https://github.com/InternLM/tutorial)ã€[**InternStudio**](https://studio.intern-ai.org.cn/)ã€[**xtuner**](https://github.com/InternLM/xtuner)ã€[**InternLM-Math**](https://github.com/InternLM/InternLM-Math)

<a href="https://github.com/AXYZdong/AMchat/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=AXYZdong/AMchat" />
</a>

</div>

## ğŸ–Šï¸ Citation

```bibtex
@misc{2024AMchat,
    title={AMchat: A large language model integrating advanced math concepts, exercises, and solutions},
    author={AMchat Contributors},
    howpublished = {\url{https://github.com/AXYZdong/AMchat}},
    year={2024}
}
```

## å¼€æºè®¸å¯è¯

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache License 2.0 å¼€æºè®¸å¯è¯](https://github.com/AXYZdong/AMchat/blob/main/LICENSE) åŒæ—¶ï¼Œè¯·éµå®ˆæ‰€ä½¿ç”¨çš„æ¨¡å‹ä¸æ•°æ®é›†çš„è®¸å¯è¯ã€‚
